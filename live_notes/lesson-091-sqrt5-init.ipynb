{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 \n",
    "\n",
    "#### How to Train your model\n",
    "\n",
    "<img src=\"https://snag.gy/GxVakf.jpg\" width=\"600px\"/>\n",
    "\n",
    "#### Recap from last Lesson\n",
    "\n",
    "- We looked at Conv2D, and looked at how it initializes parameters\n",
    "- We found `math.sqrt(5)` and didn't know the reasoning\n",
    "- How I researched: [fastai nb link](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/02a_why_sqrt5.ipynb)\n",
    "- Step 1: loaded everything we had from last week\n",
    "- Step 2: used MNIST\n",
    "- Step 3: setup a Conv2D layer\n",
    "- Step 4: looked at the first 100 samples\n",
    "- Step 5: look at the mean / std of different weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai_lib import *\n",
    "\n",
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train,y_train,x_valid,y_valid))\n",
    "\n",
    "def normalize(x, m, s): \n",
    "    return (x-m)/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why SQRT(5)?\n",
    "\n",
    "`torch.nn.modules.conv._ConvNd.reset_parameters??`\n",
    "\n",
    "    Signature: torch.nn.modules.conv._ConvNd.reset_parameters(self)\n",
    "    Docstring: <no docstring>\n",
    "    Source:   \n",
    "        def reset_parameters(self):\n",
    "            n = self.in_channels\n",
    "            init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "            if self.bias is not None:\n",
    "                fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(self.bias, -bound, bound)\n",
    "    File:      ~/envs/py3/lib/python3.6/site-packages/torch/nn/modules/conv.py\n",
    "    Type:      function\n",
    "    \n",
    "    \n",
    "To test out and understand the reasoning behind the square root 5, we will start with MNIST data and observe how the initialization affects the performance. The following code block will load and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, tensor(10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our data from MNIST\n",
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "# find our mean and std\n",
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "\n",
    "# normalize the training and the validation data\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)\n",
    "\n",
    "# because the data comes in single vectors, we\n",
    "# will reshape to the 28 x 28 sized 1 channel format images\n",
    "x_train = x_train.view(-1,1,28,28)\n",
    "x_valid = x_valid.view(-1,1,28,28)\n",
    "x_train.shape, x_valid.shape\n",
    "\n",
    "# get the number of samples\n",
    "n, *_ = x_train.shape\n",
    "\n",
    "# number of classes\n",
    "c = y_train.max()+1\n",
    "\n",
    "# number of hidden units\n",
    "nh = 32\n",
    "n, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's experiment with the Conv2d\n",
    "\n",
    "We will make a single conv2d layer, with 1 channel, and a 5x5 kernel"
   ]
  },
  {
   